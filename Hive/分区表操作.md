# 分区表操作

在 hive Select 查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，因此建表时引入了 partition 分区 概念。分区表指的是在创建表时指定的 partition 的分区空间。一个表可以拥有一个 或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。表和列名不 区分大小写。分区是以字段的形式在表结构中存在，通过 describe table 命令可 以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。


**分区表字段不能是表中已存在的字段**

**分区字段是一个虚拟的字段，不存放任何数据**

**分区字段的数据来自装载分区表数据的时候指定的**

总的说来partition就是辅助查询，缩小查询范围，加快数据的检索速度和对数据按照一定的规格和条件进行管理。


###单分区建表语句

```
create table t_user (id int, name string) partitioned by (country string) row format delimited fields terminated by ',';   ---指定分隔符创建分区表
```

新建数据文件，5.txt

```
[root@node1 hivedata]# vi 5.txt
1,rose
2,herey
3,germs
4,alfred
5,grance

```
上传到hadoop上

```
[root@node1 hivedata]# hadoop fs -put 5.txt /user/hive/warehouse/itcast.db/t_user
```

通过传统的语句 select * from t_tuser;无法查看到导入的数据，需要用下面的方式查看

删除5.txt文件内容

```
[root@node1 hivedata]# hadoop fs -rm /user/hive/warehouse/itcast.db/t_user/5.txt

18/02/12 00:31:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.

Deleted /user/hive/warehouse/itcast.db/t_user/dd.txt

[root@node1 hivedata]# 

```

然后在node2上的hive程序中执行如下命令（5.txt文件保存在node1上，node2的hive远程连接到了node1上的hive程序）

>LOAD DATA local INPATH '/root/hivedata/5.txt' INTO TABLE t_user partition(country='USA');

```
0: jdbc:hive2://node1:10000> LOAD DATA local INPATH '/root/hivedata/5.txt' INTO TABLE t_user partition(country='USA');
INFO  : Loading data to table itcast.t_user partition (country=USA) from file:/root/hivedata/5.txt
INFO  : Partition itcast.t_user{country=USA} stats: [numFiles=1, numRows=0, totalSize=41, rawDataSize=0]
No rows affected (1.416 seconds)
0: jdbc:hive2://node1:10000> select * from t_user;
+------------+--------------+-----------------+--+
| t_user.id  | t_user.name  | t_user.country  |
+------------+--------------+-----------------+--+
| 1          | rose         | USA             |
| 2          | herey        | USA             |
| 3          | germs        | USA             |
| 4          | alfred       | USA             |
| 5          | grance       | USA             |
+------------+--------------+-----------------+--+
5 rows selected (0.291 seconds)
0: jdbc:hive2://node1:10000> 

```

再次向t_user导入数据

```
[root@node1 hivedata]# vi 6.txt
6,zhangsan
7,lisi
8,zhaoliu
9,hanhan6,zhangsan

```

###导入并查看

>LOAD DATA local INPATH '/root/hivedata/6.txt' INTO TABLE t_user partition(country='China');

```
0: jdbc:hive2://node1:10000> LOAD DATA local INPATH '/root/hivedata/6.txt' INTO TABLE t_user partition(country='China');

INFO  : Loading data to table itcast.t_user partition (country=China) from file:/root/hivedata/6.txt

INFO  : Partition itcast.t_user{country=China} stats: [numFiles=1, numRows=0, totalSize=47, rawDataSize=0]
No rows affected (0.79 seconds)

0: jdbc:hive2://node1:10000> select * from t_user;
+------------+--------------+-----------------+--+
| t_user.id  | t_user.name  | t_user.country  |
+------------+--------------+-----------------+--+
| 6          | zhangsan     | China           |
| 7          | lisi         | China           |
| 8          | zhaoliu      | China           |
| 9          | hanhan6      | China           |
| 1          | rose         | USA             |
| 2          | herey        | USA             |
| 3          | germs        | USA             |
| 4          | alfred       | USA             |
| 5          | grance       | USA             |
+------------+--------------+-----------------+--+
9 rows selected (0.193 seconds)
0: jdbc:hive2://node1:10000> 

```

![](http://p2ehgqigv.bkt.clouddn.com/18-2-26/36688573.jpg)


通过分区字段查看数据

```
0: jdbc:hive2://node1:10000> select * from t_user where country='China';
+------------+--------------+-----------------+--+
| t_user.id  | t_user.name  | t_user.country  |
+------------+--------------+-----------------+--+
| 6          | zhangsan     | China           |
| 7          | lisi         | China           |
| 8          | zhaoliu      | China           |
| 9          | hanhan6      | China           |
+------------+--------------+-----------------+--+
```


###双分区建表语句

双分区表，按天和小时分区，在表结构中新增加了dt和hour两列,并指定分隔符

```
create table day_hour_table (id int, content string) partitioned by (dt string, hour string) row format delimited fields terminated by ',';
```


导入数据

>LOAD DATA local INPATH '/root/hivedata/5.txt' INTO TABLE day_hour_table partition(dt='20180201',hour='06');

```
0: jdbc:hive2://node1:10000> LOAD DATA local INPATH '/root/hivedata/6.txt' INTO TABLE day_hour_table partition(dt='20180201',hour='07');

INFO  : Loading data to table itcast.day_hour_table partition (dt=20180201, hour=07) from file:/root/hivedata/6.txt

INFO  : Partition itcast.day_hour_table{dt=20180201, hour=07} stats: [numFiles=1, numRows=0, totalSize=47, rawDataSize=0]

No rows affected (0.678 seconds)

```

###查看

```
0: jdbc:hive2://node1:10000> select * from day_hour_table;
+--------------------+-------------------------+--------------------+----------------------+--+
| day_hour_table.id  | day_hour_table.content  | day_hour_table.dt  | day_hour_table.hour  |
+--------------------+-------------------------+--------------------+----------------------+--+
| 1                  | rose                    | 20180201           | 06                   |
| 2                  | herey                   | 20180201           | 06                   |
| 3                  | germs                   | 20180201           | 06                   |
| 4                  | alfred                  | 20180201           | 06                   |
| 5                  | grance                  | 20180201           | 06                   |
+--------------------+-------------------------+--------------------+----------------------+--+
```

<!--
create time: 2018-02-26 18:30:04
Author: Alfred

This file is created by Marboo<http://marboo.io> template file $MARBOO_HOME/.media/starts/default.md
本文件由 Marboo<http://marboo.io> 模板文件 $MARBOO_HOME/.media/starts/default.md 创建
-->

